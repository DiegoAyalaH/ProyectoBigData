{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Portada\n",
        "Avance de proyecto 1: Sistema de Recomendación\n",
        "\n",
        "- A01176833 Leonardo Segura                \n",
        "- A01657027 Esteban Hidekel Solares Orozco\n",
        "- A01376727 Diego Armando Ayala Hernández  \n",
        "\n",
        "Análisis de grandes volúmenes de datos (Gpo 10)\n",
        "\n",
        "Github: https://github.com/DiegoAyalaH/ProyectoBigData\n"
      ],
      "metadata": {
        "id": "W6qIpDB7UUt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código"
      ],
      "metadata": {
        "id": "yDub4rAAT_AT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías y configuración"
      ],
      "metadata": {
        "id": "duV2GWkeUFoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar surprise\n",
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsr8lxu3HQN6",
        "outputId": "989f3dfe-dd6b-445b-9e9c-b160272d3b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357254 sha256=22dbbcf9f27fb8166a881677910228c73e4404b67bb9e2e0020eb62a430e6360\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from surprise import Reader, Dataset, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "S8MPsGdOHMJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Montamos Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPliBERTHynM",
        "outputId": "9ebd86da-5b8e-489d-ca8d-1eedee14814b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path de carpeta de proyecto\n",
        "drivepath = \"/content/drive/MyDrive/4034 Big Data/4034 Big Data Equipo 4/Recomendaciones libros/\""
      ],
      "metadata": {
        "id": "m2DgmcQWH1Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-procesamiento y transformacione\n",
        "En esta libreta, hemos seleccionado un conjunto de datos de calificaciones de libros, ya que es ideal para construir y evaluar sistemas de recomendación. Este tipo de sistemas es fundamental en la industria del comercio electrónico y entretenimiento, donde personalizan la experiencia del usuario.\n",
        "\n",
        "Comenzamos importando las bibliotecas necesarias y leyendo los conjuntos de datos de libros, calificaciones y usuarios. Luego, fusionamos las calificaciones con la información de los libros y eliminamos las columnas de imágenes, ya que no son necesarias para nuestro análisis. Renombramos las columnas para mayor claridad y iltramos los usuarios que han dado al menos 50 calificaciones y los libros que han recibido al menos 10 calificaciones para asegurar suficiente variabilidad.\n"
      ],
      "metadata": {
        "id": "bktr6K0DLYWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer los conjuntos de datos\n",
        "libros = pd.read_csv(drivepath+\"Books.csv\")\n",
        "calificaciones = pd.read_csv(drivepath+\"Ratings.csv\")\n",
        "usuarios = pd.read_csv(drivepath+\"Users.csv\")\n",
        "\n",
        "# Fusionar conjuntos de datos de calificaciones y libros\n",
        "df = pd.merge(calificaciones, libros.drop_duplicates(['ISBN']), on=\"ISBN\", how=\"left\")\n",
        "df.drop(['Image-URL-S','Image-URL-M','Image-URL-L'], axis=1, inplace=True)\n",
        "\n",
        "# Renombrar columnas\n",
        "df.rename(columns={'User-ID':'user_id', 'ISBN':'book_id', 'Book-Rating':'rating'}, inplace=True)\n",
        "df['book_id'] = df['book_id'].astype(str)\n",
        "\n",
        "# Filtrar usuarios con al menos 50 calificaciones y libros con al menos 10 calificaciones\n",
        "limite_calificaciones_usuario = 50\n",
        "limite_calificaciones_libro = 10\n",
        "\n",
        "conteo_calificaciones_usuario = df['user_id'].value_counts()\n",
        "conteo_calificaciones_libro = df['book_id'].value_counts()\n",
        "\n",
        "usuarios_a_eliminar = conteo_calificaciones_usuario[conteo_calificaciones_usuario < limite_calificaciones_usuario].index\n",
        "libros_a_eliminar = conteo_calificaciones_libro[conteo_calificaciones_libro < limite_calificaciones_libro].index\n",
        "\n",
        "df = df[~df['user_id'].isin(usuarios_a_eliminar)]\n",
        "df = df[~df['book_id'].isin(libros_a_eliminar)]\n"
      ],
      "metadata": {
        "id": "QpkpUCChHMEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis exploratorio\n",
        "En la sección de análisis exploratorio, primero mostramos las primeras filas del conjunto de datos para tener una visión general de su estructura y contenido. Luego, verificamos la distribución de las calificaciones utilizando un gráfico de barras. Esto nos ayuda a entender cómo se distribuyen las calificaciones en el conjunto de datos, identificando posibles sesgos o patrones que  podrían influir en el rendimiento del modelo de recomendación.\n",
        "\n",
        "Eliminamos las calificaciones con valor 0, ya que no representan una calificación válida sino la ausencia de una. Podemos ver que la calificación de 8 es la más común (a excepción de la falta de calificación)."
      ],
      "metadata": {
        "id": "_fLVo5X9LcGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras filas del conjunto de datos\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AB4H-poHHLzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar la distribución de calificaciones\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.countplot(x=\"rating\", data=df)\n",
        "plt.xlabel(\"Calificaciones\")\n",
        "plt.ylabel(\"Número de Calificaciones\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "efxfFlIGHb2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar calificaciones de 0\n",
        "df = df[df['rating'] != 0]\n",
        "\n",
        "# Verificar la distribución de calificaciones\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.countplot(x=\"rating\", data=df)\n",
        "plt.xlabel(\"Calificaciones\")\n",
        "plt.ylabel(\"Número de Calificaciones\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8iJ-whP1KIVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificado\n",
        "Para preparar los datos para el modelo, codificamos las columnas user_id y book_id utilizando LabelEncoder, lo cual transforma  los identificadores en un formato numérico necesario para el modelo. Luego, utilizamos la clase Reader de la biblioteca surprise  para definir la escala de calificación esperada y cargar el conjunto de datos en el formato requerido por surprise. Dividimos  los datos en conjuntos de entrenamiento y prueba para evaluar el modelo de manera adecuada.\n"
      ],
      "metadata": {
        "id": "yRot_WEcNtO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar user_id y book_id\n",
        "encoder = LabelEncoder()\n",
        "datos = df[['user_id', 'book_id']].apply(encoder.fit_transform)\n",
        "datos['rating'] = df['rating']\n",
        "\n",
        "# Preparar el conjunto de datos para la biblioteca surprise\n",
        "lector = Reader(rating_scale=(1, 10))\n",
        "dataset_surprise = Dataset.load_from_df(datos[['user_id', 'book_id', 'rating']], lector)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "trainset, testset = train_test_split(dataset_surprise, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "gKEx7XvyHfbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo básico (KNN basado en similitud de coseno)\n",
        "En esta sección, implementamos el modelo de recomendación básico utilizando KNN basado en la similitud del coseno. Definimos las opciones de similitud sin el parámetro min_support para evitar problemas con vecinos insuficientes. Entrenamos el algoritmo en  el conjunto de entrenamiento y luego evaluamos su rendimiento en el conjunto de prueba. Utilizamos métricas de precisión, recall  y F1-score para evaluar la calidad de las recomendaciones.\n"
      ],
      "metadata": {
        "id": "tzxelVGULsBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir las opciones de similitud con min_support\n",
        "sim_options = {'name': 'cosine', 'user_based': True}\n",
        "\n",
        "# Crear el algoritmo KNN básico\n",
        "algo_knn_user = KNNBasic(sim_options=sim_options, verbose=False)\n",
        "\n",
        "# Entrenar el algoritmo en el conjunto de entrenamiento\n",
        "algo_knn_user.fit(trainset)"
      ],
      "metadata": {
        "id": "s8Htps8pHhJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación\n",
        "Para evaluar el modelo, definimos una función precision_recall_at_k que calcula la precisión, el recall y la puntuación F1. Esta función agrupa las predicciones por usuario, ordena las calificaciones estimadas y calcula las métricas en función de un umbral definido. Imprimimos las métricas calculadas y utilizamos la función rmse de surprise para calcular el error cuadrático medio de las predicciones. Esto nos proporciona una visión completa del rendimiento del modelo.\n",
        "\n",
        "Los resultados muestran un RMSE de 1.9551, lo que indica la desviación promedio de las calificaciones predichas respecto a las calificaciones reales. La precisión es de 0.794, lo que significa que el 79.4% de las recomendaciones fueron relevantes. El recall es de 0.787, lo que indica que el 78.7% de los ítems relevantes fueron recomendados. La puntuación F1, que es una medida combinada de precisión y recall, es de 0.79."
      ],
      "metadata": {
        "id": "dL-ctLPJOXGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para calcular precisión, recall y F1-score\n",
        "def precision_recall_at_k(model, k=10, threshold=7):\n",
        "    # Diccionario para almacenar las predicciones de cada usuario\n",
        "    user_est_true = defaultdict(list)\n",
        "    # Obtener predicciones para el conjunto de prueba\n",
        "    predictions = model.test(testset)\n",
        "    # Agrupar las predicciones por usuario\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    # Calcular precisión y recall para cada usuario\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "        # Ordenar las calificaciones del usuario por el valor estimado\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "        # Número de ítems relevantes\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "        # Número de ítems recomendados en el top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "        # Número de ítems relevantes y recomendados en el top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold)) for (est, true_r) in user_ratings[:k])\n",
        "        # Precisión@K: Proporción de ítems recomendados que son relevantes\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "        # Recall@K: Proporción de ítems relevantes que son recomendados\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "    # Calcular la media de todas las precisiones predichas\n",
        "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
        "    # Calcular la media de todos los recalls predichos\n",
        "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
        "    # Calcular RMSE\n",
        "    accuracy.rmse(predictions)\n",
        "    # Imprimir precisión, recall y F1-score\n",
        "    print('Precisión: ', precision)\n",
        "    print('Recall: ', recall)\n",
        "    print('Puntuación F_1: ', round((2*precision*recall)/(precision+recall), 3))\n",
        "\n",
        "# Calcular precisión, recall y F1-score\n",
        "precision_recall_at_k(algo_knn_user)\n"
      ],
      "metadata": {
        "id": "afPi-889Hiku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción"
      ],
      "metadata": {
        "id": "HLqZP_-QOzQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir la calificación para un usuario y un libro específico\n",
        "algo_knn_user.predict(69, 420, r_ui=None, verbose=True)"
      ],
      "metadata": {
        "id": "JuSnhJmYHno7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, realizamos una predicción específica para un usuario y un libro, utilizando el modelo entrenado. Esto nos permite ver una predicción real y evaluar cómo el modelo maneja casos específicos. Esta predicción se compara con una calificación real (si está disponible) o se estima una nueva calificación. Esta sección demuestra la aplicabilidad práctica del modelo de recomendación.\n",
        "\n",
        "En este caso, la predicción para el usuario con ID 69 y el libro con ID 420 es una calificación estimada de 10.00. Esto indica que el modelo predice que este usuario otorgará una calificación alta a este libro.\n"
      ],
      "metadata": {
        "id": "uEPDRqqLOq8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los parámetros\n",
        "x = 10  # Número de calificaciones a mostrar\n",
        "n = 42   # Iniciar desde la n-ésima combinación válida\n",
        "\n",
        "# Lista para almacenar las combinaciones de calificaciones reales y predichas\n",
        "predictions_list = []\n",
        "\n",
        "# Contador para rastrear combinaciones válidas\n",
        "valid_count = 0\n",
        "\n",
        "# Recorrer el conjunto de prueba para encontrar combinaciones válidas\n",
        "for (u, i, r) in testset:\n",
        "    if r is not None:\n",
        "        valid_count += 1\n",
        "        # Comenzar a recopilar combinaciones después de la n-ésima combinación válida\n",
        "        if valid_count >= n:\n",
        "            prediction = algo_knn_user.predict(u, i, r_ui=r, verbose=False)\n",
        "            predictions_list.append((u, i, r, prediction.est))\n",
        "            # Detener la recopilación después de encontrar x combinaciones\n",
        "            if len(predictions_list) == x:\n",
        "                break\n",
        "\n",
        "# Crear un DataFrame para mostrar los resultados\n",
        "predictions_df = pd.DataFrame(predictions_list, columns=['user_id', 'book_id', 'calificación_real', 'calificación_predicha'])\n",
        "print(predictions_df)"
      ],
      "metadata": {
        "id": "NRXAkQAOO88T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, realizamos predicciones específicas para varios usuarios y libros, utilizando el modelo entrenado. Esto nos permite ver múltiples predicciones reales y evaluar cómo el modelo maneja diferentes casos específicos. Recopilamos una lista de `x` combinaciones válidas de calificaciones reales y predichas, comenzando desde la `n`-ésima combinación válida. Esto nos proporciona una visión más amplia del rendimiento del modelo en diferentes escenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "GBLnme4MPFYA"
      }
    }
  ]
}